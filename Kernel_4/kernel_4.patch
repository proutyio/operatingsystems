diff --git a/arch/x86/syscalls/syscall_32.tbl b/arch/x86/syscalls/syscall_32.tbl
index b3560ec..07205cf 100644
--- a/arch/x86/syscalls/syscall_32.tbl
+++ b/arch/x86/syscalls/syscall_32.tbl
@@ -365,3 +365,5 @@
 356	i386	memfd_create		sys_memfd_create
 357	i386	bpf			sys_bpf
 358	i386	execveat		sys_execveat			stub32_execveat
+359	i386	free_slob		sys_free_slob
+360	i386	slob_used		sys_slob_used
\ No newline at end of file
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index 85893d7..ada9294 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -882,4 +882,7 @@ asmlinkage long sys_execveat(int dfd, const char __user *filename,
 			const char __user *const __user *argv,
 			const char __user *const __user *envp, int flags);
 
+asmlinkage long sys_free_slob(void);
+asmlinkage long sys_slob_used(void);
+
 #endif
diff --git a/mm/slob.c b/mm/slob.c
index 96a8620..4a0bab6 100644
--- a/mm/slob.c
+++ b/mm/slob.c
@@ -101,6 +101,10 @@ static LIST_HEAD(free_slob_small);
 static LIST_HEAD(free_slob_medium);
 static LIST_HEAD(free_slob_large);
 
+/* Initilize page tracking variables */
+unsigned long slob_pages = 0;
+unsigned long slob_free_units = 0;
+
 /*
  * slob_page_free: true for pages on free_slob_pages list.
  */
@@ -206,7 +210,7 @@ static void *slob_new_pages(gfp_t gfp, int order, int node)
 
 static void slob_free_pages(void *b, int order)
 {
-	if (current->reclaim_state)
+  if (current->reclaim_state)
 		current->reclaim_state->reclaimed_slab += 1 << order;
 	free_pages((unsigned long)b, order);
 }
@@ -216,50 +220,73 @@ static void slob_free_pages(void *b, int order)
  */
 static void *slob_page_alloc(struct page *sp, size_t size, int align)
 {
-	slob_t *prev, *cur, *aligned = NULL;
-	int delta = 0, units = SLOB_UNITS(size);
+  slob_t *prev, *cur, *aligned = NULL;
+  int delta = 0, units = SLOB_UNITS(size);
+  
+  /* Best-fit variables */
+  slob_t *bf_prev, *bf_cur, *bf_aligned = NULL;
+  int bf_delta = 0;
+  slobidx_t bf_diff = 0;
+
+  for (prev = NULL, cur = sp->freelist; ; prev = cur, cur = slob_next(cur)) {
+    slobidx_t avail = slob_units(cur);
+    
+    if (align) {
+      aligned = (slob_t *)ALIGN((unsigned long)cur, align);
+      delta = aligned - cur;
+    }
+    
+    /* Check if enough room */
+    if (avail >= units + delta) {
+      /* Check if best-fit variables need update or inititialization */
+      if ((avail - (units + delta)) < bf_diff || bf_cur == NULL) { 
+	bf_prev = prev;
+	bf_cur = cur;
+	bf_aligned = aligned;
+	bf_delta = delta;
+	bf_diff = avail - (units + delta);
+      }
+    }
+    
+    /* Check if at end of list */
+    if(slob_last(cur)) {
+      if(bf_cur != NULL) {
+	slob_t *bf_next = NULL;
+	slobidx_t bf_avail = slob_units(bf_cur);
+	
+	if(bf_delta) {
+	  bf_next = slob_next(bf_cur);
+	  set_slob(bf_aligned, bf_avail - bf_delta, bf_next);
+	  set_slob(bf_cur, bf_delta, bf_aligned);
+	  bf_prev = bf_cur;
+	  bf_cur = bf_aligned;
+	  bf_avail = slob_units(bf_cur);
+	}
+	
+	bf_next = slob_next(bf_cur);
+	if (bf_avail == units) { /* exact fit? unlink. */
+	  if (bf_prev)
+	    set_slob(bf_prev, slob_units(bf_prev), bf_next);
+	  else
+	    sp->freelist = bf_next;
+	} else { /* fragment */
+	  if (bf_prev)
+	    set_slob(bf_prev, slob_units(bf_prev), bf_cur + units);
+	  else
+	    sp->freelist = bf_cur + units;
+	  
+	  set_slob(bf_cur + units, bf_avail - units, bf_next);
+	}
 
-	for (prev = NULL, cur = sp->freelist; ; prev = cur, cur = slob_next(cur)) {
-		slobidx_t avail = slob_units(cur);
+	sp->units -= units;
+	if (!sp->units)
+	  clear_slob_page_free(sp);
+	return bf_cur;
+      }
 
-		if (align) {
-			aligned = (slob_t *)ALIGN((unsigned long)cur, align);
-			delta = aligned - cur;
-		}
-		if (avail >= units + delta) { /* room enough? */
-			slob_t *next;
-
-			if (delta) { /* need to fragment head to align? */
-				next = slob_next(cur);
-				set_slob(aligned, avail - delta, next);
-				set_slob(cur, delta, aligned);
-				prev = cur;
-				cur = aligned;
-				avail = slob_units(cur);
-			}
-
-			next = slob_next(cur);
-			if (avail == units) { /* exact fit? unlink. */
-				if (prev)
-					set_slob(prev, slob_units(prev), next);
-				else
-					sp->freelist = next;
-			} else { /* fragment */
-				if (prev)
-					set_slob(prev, slob_units(prev), cur + units);
-				else
-					sp->freelist = cur + units;
-				set_slob(cur + units, avail - units, next);
-			}
-
-			sp->units -= units;
-			if (!sp->units)
-				clear_slob_page_free(sp);
-			return cur;
-		}
-		if (slob_last(cur))
-			return NULL;
-	}
+      return NULL;
+    }
+  }
 }
 
 /*
@@ -270,8 +297,10 @@ static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 	struct page *sp;
 	struct list_head *prev;
 	struct list_head *slob_list;
+	struct list_head *curr;
 	slob_t *b = NULL;
 	unsigned long flags;
+	slob_free_units = 0;
 
 	if (size < SLOB_BREAK1)
 		slob_list = &free_slob_small;
@@ -309,8 +338,25 @@ static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 			list_move_tail(slob_list, prev->next);
 		break;
 	}
-	spin_unlock_irqrestore(&slob_lock, flags);
 
+	/* Iterate through lists to find free space */
+	curr = &free_slob_large;
+	list_for_each_entry(sp, curr, lru) {
+	  slob_free_units += sp->units;
+	}
+
+	curr = &free_slob_medium;
+	list_for_each_entry(sp, curr, lru) {
+	  slob_free_units += sp->units;
+	}
+
+	curr = &free_slob_small;
+	list_for_each_entry(sp, curr, lru) {
+	  slob_free_units += sp->units;
+	}
+
+	spin_unlock_irqrestore(&slob_lock, flags);
+	
 	/* Not enough space: must allocate a new page */
 	if (!b) {
 		b = slob_new_pages(gfp & ~__GFP_ZERO, 0, node);
@@ -328,6 +374,9 @@ static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 		b = slob_page_alloc(sp, size, align);
 		BUG_ON(!b);
 		spin_unlock_irqrestore(&slob_lock, flags);
+		
+		/* Increment page count*/
+		slob_pages++;
 	}
 	if (unlikely((gfp & __GFP_ZERO) && b))
 		memset(b, 0, size);
@@ -362,6 +411,9 @@ static void slob_free(void *block, int size)
 		__ClearPageSlab(sp);
 		page_mapcount_reset(sp);
 		slob_free_pages(b, 0);
+
+		/* Decrement page count */
+		slob_pages--;
 		return;
 	}
 
@@ -640,3 +692,14 @@ void __init kmem_cache_init_late(void)
 {
 	slab_state = FULL;
 }
+
+asmlinkage long sys_slob_used(void) 
+{
+  long slob_used_total = SLOB_UNITS(PAGE_SIZE) * slob_pages;
+  return slob_used_total;
+}
+
+asmlinkage long sys_free_slob(void)
+{
+  return slob_free_units;
+}
